.. note::

    Hallo und willkommen in der SunFounder Raspberry Pi & Arduino & ESP32 Enthusiasten-Gemeinschaft auf Facebook! Tauchen Sie tiefer ein in die Welt von Raspberry Pi, Arduino und ESP32 mit anderen Enthusiasten.

    **Warum beitreten?**

    - **Expertenunterst√ºtzung**: L√∂sen Sie Nachverkaufsprobleme und technische Herausforderungen mit Hilfe unserer Gemeinschaft und unseres Teams.
    - **Lernen & Teilen**: Tauschen Sie Tipps und Anleitungen aus, um Ihre F√§higkeiten zu verbessern.
    - **Exklusive Vorschauen**: Erhalten Sie fr√ºhzeitigen Zugang zu neuen Produktank√ºndigungen und exklusiven Einblicken.
    - **Spezialrabatte**: Genie√üen Sie exklusive Rabatte auf unsere neuesten Produkte.
    - **Festliche Aktionen und Gewinnspiele**: Nehmen Sie an Gewinnspielen und Feiertagsaktionen teil.

    üëâ Sind Sie bereit, mit uns zu erkunden und zu erschaffen? Klicken Sie auf [|link_sf_facebook|] und treten Sie heute bei!

.. _ai_voice_assistant_robot:

20. KI-Sprachassistent-Hund
===========================

In dieser Lektion verwandelst du deinen Pidog in einen **KI-gest√ºtzten Sprachassistenten-Hund** üê∂.  
Der Roboter kann auf deine Stimme reagieren, verstehen, was du sagst, mit Pers√∂nlichkeit antworten  
und seine ‚ÄûGef√ºhle‚Äú durch Bewegungen, Gesten und LED-Lichteffekte ausdr√ºcken.

Du wirst einen **voll interaktiven Robotergef√§hrten** bauen mit:

* **LLM**: Large Language Model (z. B. OpenAI GPT oder Doubao) f√ºr nat√ºrliche Konversation.  
* **STT**: Speech-to-Text f√ºr Spracherkennung.  
* **TTS**: Text-to-Speech f√ºr ausdrucksstarke Sprachausgabe.  
* **Sensoren + Aktionen**: Ultraschallsensorik, Kamerasicht (optional), Ber√ºhrungssensoren und eingebaute ausdrucksstarke Bewegungen.

----

Bevor du beginnst
-----------------

Stelle sicher, dass du Folgendes abgeschlossen hast:

* :ref:`install_all_modules` ‚Äî Installiere die Module ``robot-hat``, ``vilib``, ``pidog`` und f√ºhre dann das Skript ``i2samp.sh`` aus.
* :ref:`test_piper` ‚Äî √úberpr√ºfe die unterst√ºtzten Sprachen von **Piper TTS**.  
* :ref:`test_vosk` ‚Äî √úberpr√ºfe die unterst√ºtzten Sprachen von **Vosk STT**.  
* :ref:`py_online_llm` ‚Äî Dieser Schritt ist **sehr wichtig**: Besorge dir deinen **OpenAI**- oder **Doubao**-API-Schl√ºssel oder den API-Schl√ºssel eines anderen unterst√ºtzten LLM.

Du solltest bereits haben:

* Ein funktionierendes **Mikrofon** und **Lautsprecher** an deinem Pidog.  
* Einen **g√ºltigen API-Schl√ºssel**, der in ``secret.py`` gespeichert ist.  
* Eine stabile Netzwerkverbindung (eine **Kabelverbindung** wird f√ºr bessere Stabilit√§t empfohlen).

----

Beispiel ausf√ºhren
------------------

Beide Sprachversionen befinden sich im selben Verzeichnis:

.. code-block:: bash

   cd ~/pidog/examples

**Englische Version** (OpenAI GPT, Anweisungen auf Englisch):

.. code-block:: bash

   sudo python3 20_voice_active_dog_gpt.py

* LLM: ``OpenAI GPT-4o-mini``  
* TTS: ``en_US-ryan-low`` (Piper)  
* STT: Vosk (``en-us``)

Aktivierungswort:

.. code-block::

   "Hey buddy"

---

**Chinesische Version** (Doubao, Anweisungen auf Chinesisch):

.. code-block:: bash

   sudo python3 20_voice_active_dog_doubao_cn.py

* LLM: ``Doubao-seed-1-6-250615``  
* TTS: ``zh_CN-huayan-x_low`` (Piper)  
* STT: Vosk (``cn``)

Aktivierungswort:

.. code-block::

   "‰Ω†Â•Ω Êó∫Ë¥¢"

.. note::

   Du kannst das **Aktivierungswort** und den **Roboternamen** im Code √§ndern:
   ``NAME = "Buddy"`` oder ``NAME = "Êó∫Ë¥¢"``  
   ``WAKE_WORD = ["hey buddy"]`` oder ``WAKE_WORD = ["‰Ω†Â•Ω Êó∫Ë¥¢"]``

----

Was passieren wird
------------------

Wenn du dieses Beispiel erfolgreich ausf√ºhrst:

* Der Roboter **wartet auf das Aktivierungswort** (z. B. ‚ÄûHey Buddy‚Äú, ‚Äû‰Ω†Â•Ω Êó∫Ë¥¢‚Äú).  
* Wenn er das Aktivierungswort h√∂rt:

  * Der LED-Streifen leuchtet **rosa (Atmungseffekt)** als Aufwachsignal.  
  * Der Roboter begr√º√üt dich mit der eingestellten Aufwach-Antwort ‚Äî  
    z. B. ‚ÄûHi there!‚Äú (√ºber Piper TTS).

* Anschlie√üend beginnt er **deiner Stimme zuzuh√∂ren** √ºber Vosk STT (oder akzeptiert Tastatureingaben, wenn aktiviert).

* Nachdem erkannt wurde, was du gesagt hast, f√ºhrt das System Folgendes aus:

  * Nimmt ein **Kamerabild** auf (weil ``WITH_IMAGE = True``) und sendet deine Nachricht + Bild an das **LLM** (OpenAI ``gpt-4o-mini``).  
  * LED wechselt zu **gelb (h√∂rt/verarbeitet)**, w√§hrend das Modell denkt.  
  * Die Antwort des Modells wird in zwei Teile aufgeteilt:

    - Text vor ``ACTIONS:`` ‚Üí wird laut ausgesprochen.  
    - Schl√ºsselw√∂rter nach ``ACTIONS:`` ‚Üí werden den Roboterbewegungen zugeordnet.

  * Der Roboter **f√ºhrt diese Aktionen aus** √ºber ``ActionFlow``.  
  * Nach Abschluss der Aktionen **kehrt der Roboter in die SIT-Haltung zur√ºck** und schaltet die LEDs aus.

* Wenn der **Ultraschallsensor ein Hindernis** n√§her als 10 cm erkennt:

  - Eine Nachricht wird eingef√ºgt: ``<<<Ultrasonic sense too close: {distance}cm>>>``  
  - Der Roboter f√§hrt automatisch zur√ºck: ``ACTIONS: backward``  
  - **Bildeingabe ist** f√ºr diese Runde **deaktiviert**.

* Wenn der **Ber√ºhrungssensor ausgel√∂st wird**:

  - Bei einer **LIKE**-Ber√ºhrung (z. B. FRONT_TO_REAR):

    - Einf√ºgen: ``<<<Touch style you like: FRONT_TO_REAR>>>``  
    - ``ACTIONS: nod`` (positive Reaktion)

  - Bei einer **HATE**-Ber√ºhrung (z. B. REAR_TO_FRONT):

    - Einf√ºgen: ``<<<Touch style you hate: REAR_TO_FRONT>>>``  
    - ``ACTIONS: backward`` (Abwehrreaktion)

* **LED-Lebenszyklus:**

  - ``on_start`` ‚Üí SIT-Haltung, LEDs aus  
  - ``before_listen`` ‚Üí cyan (bereit)  
  - ``before_think`` ‚Üí gelb (verarbeiten)  
  - ``before_say`` ‚Üí rosa (spricht)  
  - ``after_say`` / ``on_finish_a_round`` ‚Üí SIT-Haltung, LEDs aus  
  - ``on_stop`` ‚Üí Aktionsfluss stoppen und Ger√§te schlie√üen

**Beispielinteraktion**

.. code-block:: text

   You: Hey Buddy
   Robot: Hi there!

   You: What do you see in front of you?
   Robot: I can see a notebook and a blue mug on the table.
   ACTIONS: think

   You: Do a little nod for me.
   Robot: Of course. Watch my majestic nod.
   ACTIONS: nod

   (Front-to-rear touch on the head)
   Robot: Ooooh, that‚Äôs nice!
   ACTIONS: nod

   (Moving too close)
   Robot: Hey hey‚Äîtoo close! Backing up for safety.
   ACTIONS: backward


----

Wechsel zu anderen LLMs oder TTS
-----------------------------------

Du kannst ganz einfach auf andere LLMs, TTS- oder STT-Sprachen umschalten ‚Äî mit nur wenigen Code√§nderungen:

* Unterst√ºtzte LLMs:

  * OpenAI
  * Doubao
  * Deepseek
  * Gemini
  * Qwen
  * Grok

* :ref:`test_piper` ‚Äî √úberpr√ºfe die unterst√ºtzten Sprachen von **Piper TTS**.  
* :ref:`test_vosk` ‚Äî √úberpr√ºfe die unterst√ºtzten Sprachen von **Vosk STT**.  

Um umzuschalten, √§ndere einfach den Initialisierungsteil im Code:

.. code-block:: python

  from pidog.llm import OpenAI as LLM

  llm = LLM(
      api_key=API_KEY,
      model="gpt-4o-mini",
  )

  # Modelle und Sprachen festlegen
  TTS_MODEL = "en_US-ryan-low"
  STT_LANGUAGE = "en-us"

----

Fehlerbehebung
--------------

* **Der Roboter reagiert nicht auf das Aktivierungswort**

  * √úberpr√ºfe, ob das Mikrofon funktioniert.  
  * Stelle sicher, dass ``WAKE_ENABLE = True`` ist.  
  * Passe das Aktivierungswort an deine Aussprache an.

* **Kein Ton aus dem Lautsprecher**
 
  * √úberpr√ºfe die TTS-Modellkonfiguration.  
  * Teste Piper oder Espeak manuell.  
  * √úberpr√ºfe Lautsprecheranschluss und Lautst√§rke.

* **API-Key-Fehler oder Zeit√ºberschreitung** 
 
  * √úberpr√ºfe deinen Schl√ºssel in ``secret.py``.  
  * Stelle sicher, dass eine Netzwerkverbindung besteht.  
  * Best√§tige, dass das LLM unterst√ºtzt wird.

* **Ultraschallsensor wird st√§ndig unerwartet ausgel√∂st.**  

  * √úberpr√ºfe Montageh√∂he und Winkel des Sensors.  
  * Passe den ``TOO_CLOSE``-Distanzschwellenwert im Code an.
