.. note::

    隆Hola! Bienvenido a la comunidad de entusiastas de SunFounder para Raspberry Pi, Arduino y ESP32 en Facebook. Sum茅rgete en el mundo de Raspberry Pi, Arduino y ESP32 junto a otros apasionados.

    **驴Por qu茅 unirse?**

    - **Soporte experto**: Resuelve problemas postventa y desaf铆os t茅cnicos con la ayuda de nuestra comunidad y equipo.
    - **Aprender y compartir**: Intercambia consejos y tutoriales para mejorar tus habilidades.
    - **Preestrenos exclusivos**: Obt茅n acceso anticipado a nuevos anuncios de productos y adelantos exclusivos.
    - **Descuentos especiales**: Disfruta de descuentos exclusivos en nuestros productos m谩s recientes.
    - **Promociones y sorteos festivos**: Participa en sorteos y promociones especiales durante las festividades.

     驴Listo para explorar y crear con nosotros? 隆Haz clic en [|link_sf_facebook|] y 煤nete hoy mismo!

7. Rastreo Facial
======================

PiDog se sentar谩 tranquilamente en su lugar. Si aplaudes, girar谩 hacia el sonido y, si te ve, te saludar谩.

.. raw:: html

   <video width="600" loop autoplay muted>
      <source src="../_static/video/face_track.mp4" type="video/mp4">
      Your browser does not support the video tag.
   </video>

**Ejecutar el C贸digo**

.. raw:: html

    <run></run>

.. code-block::

    cd ~/pidog/examples
    sudo python3 7_face_track.py


Despu茅s de ejecutar este c贸digo, PiDog iniciar谩 la c谩mara y activar谩 la funci贸n de detecci贸n facial.
Puedes visitar ``http://+ PiDog's IP +/mjpg`` (por ejemplo: ``http://192.168.18.138:9000/mjpg``) en tu navegador para ver la imagen de la c谩mara.

Luego, PiDog se sentar谩 y activar谩 el M贸dulo de Detecci贸n de Direcci贸n del Sonido para detectar la direcci贸n de tus aplausos.
Cuando PiDog escuche aplausos (u otros ruidos), girar谩 su cabeza hacia la fuente del sonido, intentando encontrarte.

Si te ve (la detecci贸n facial encuentra un objeto), mover谩 la cola y ladrar谩 para saludarte.


**C贸digo**

.. note::
    Puedes **Modificar/Restablecer/Copiar/Ejecutar/Detener** el c贸digo a continuaci贸n. Pero antes de eso, necesitas ir a la ruta del c贸digo fuente como ``pidog\examples``. Despu茅s de modificar el c贸digo, puedes ejecutarlo directamente para ver el efecto.

.. raw:: html

    <run></run>

.. code-block:: python

    #!/usr/bin/env python3
    from pidog import Pidog
    from time import sleep
    from vilib import Vilib
    from preset_actions import bark

    my_dog = Pidog()
    sleep(0.1)

    def face_track():
        Vilib.camera_start(vflip=False, hflip=False)
        Vilib.display(local=True, web=True)
        Vilib.human_detect_switch(True)
        sleep(0.2)
        print('start')
        yaw = 0
        roll = 0
        pitch = 0
        flag = False
        direction = 0

        my_dog.do_action('sit', speed=50)
        my_dog.head_move([[yaw, 0, pitch]], pitch_comp=-40, immediately=True, speed=80)
        my_dog.wait_all_done()
        sleep(0.5)
        # Limpieza de detecci贸n de sonido moviendo servos
        if my_dog.ears.isdetected():    
            direction = my_dog.ears.read()

        while True:
            if flag == False:
                my_dog.rgb_strip.set_mode('breath', 'pink', bps=1)
            # Si escucha algo, gira para mirar
            if my_dog.ears.isdetected():
                flag = False
                direction = my_dog.ears.read()
                pitch = 0
                if direction > 0 and direction < 160:
                    yaw = -direction
                    if yaw < -80:
                        yaw = -80
                elif direction > 200 and direction < 360:
                    yaw = 360 - direction
                    if yaw > 80:
                        yaw = 80
                my_dog.head_move([[yaw, 0, pitch]], pitch_comp=-40, immediately=True, speed=80)
                my_dog.wait_head_done()
                sleep(0.05)

            ex = Vilib.detect_obj_parameter['human_x'] - 320
            ey = Vilib.detect_obj_parameter['human_y'] - 240
            people = Vilib.detect_obj_parameter['human_n']

            # Si ve a alguien, ladra para saludar
            if people > 0 and flag == False:
                flag = True
                my_dog.do_action('wag_tail', step_count=2, speed=100)
                bark(my_dog, [yaw, 0, 0], pitch_comp=-40, volume=80)
                if my_dog.ears.isdetected():
                    direction = my_dog.ears.read()

            if ex > 15 and yaw > -80:
                yaw -= 0.5 * int(ex/30.0+0.5)

            elif ex < -15 and yaw < 80:
                yaw += 0.5 * int(-ex/30.0+0.5)

            if ey > 25:
                pitch -= 1*int(ey/50+0.5)
                if pitch < - 30:
                    pitch = -30
            elif ey < -25:
                pitch += 1*int(-ey/50+0.5)
                if pitch > 30:
                    pitch = 30

            print('direction: %s |number: %s | ex, ey: %s, %s | yrp: %s, %s, %s '
                % (direction, people, ex, ey, round(yaw, 2), round(roll, 2), round(pitch, 2)),
                end='\r',
                flush=True,
                )
            my_dog.head_move([[yaw, 0, pitch]], pitch_comp=-40, immediately=True, speed=100)
            sleep(0.05)


    if __name__ == "__main__":
        try:
            face_track()
        except KeyboardInterrupt:
            pass
        except Exception as e:
            print(f"\033[31mERROR: {e}\033[m")
        finally:
            Vilib.camera_close()
            my_dog.close()
