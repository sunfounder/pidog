.. note::

    Bonjour, bienvenue dans la communaut√© SunFounder Raspberry Pi & Arduino & ESP32 Enthusiasts sur Facebook ! Plongez plus profond√©ment dans l‚Äôunivers de Raspberry Pi, Arduino et ESP32 avec d‚Äôautres passionn√©s.

    **Pourquoi rejoindre ?**

    - **Support d'experts** : R√©solvez les probl√®mes apr√®s-vente et relevez les d√©fis techniques avec l'aide de notre communaut√© et de notre √©quipe.
    - **Apprendre et partager** : √âchangez des astuces et des tutoriels pour am√©liorer vos comp√©tences.
    - **Aper√ßus exclusifs** : B√©n√©ficiez d‚Äôun acc√®s anticip√© aux annonces de nouveaux produits et √† des avant-premi√®res.
    - **R√©ductions sp√©ciales** : Profitez de remises exclusives sur nos produits les plus r√©cents.
    - **Promotions festives et concours** : Participez √† des concours et √† des promotions sp√©ciales lors des f√™tes.

    üëâ Pr√™t √† explorer et √† cr√©er avec nous ? Cliquez sur [|link_sf_facebook|] et rejoignez-nous d√®s aujourd'hui !

.. _ai_voice_assistant_robot:

20. Chien Assistant Vocal IA
================================

Cette le√ßon transforme votre Pidog en un **chien assistant vocal aliment√© par l‚ÄôIA** üê∂.  
Le robot peut se r√©veiller √† votre voix, comprendre ce que vous dites, r√©pondre avec personnalit√©,  
et exprimer ses ¬´ √©motions ¬ª √† travers des mouvements, des gestes et des effets lumineux LED.

Vous allez construire un **compagnon robotique enti√®rement interactif** en utilisant :

* **LLM** : Large Language Model (par ex. OpenAI GPT ou Doubao) pour une conversation naturelle.  
* **STT** : Speech-to-Text pour la reconnaissance vocale.  
* **TTS** : Text-to-Speech pour des r√©ponses vocales expressives.  
* **Capteurs + Actions** : D√©tection ultrasonique, vision par cam√©ra (optionnelle), capteurs tactiles et mouvements expressifs int√©gr√©s.

----

Avant de Commencer
------------------

Assurez-vous d‚Äôavoir termin√© :

* :ref:`install_all_modules` ‚Äî Installer les modules ``robot-hat``, ``vilib``, ``pidog``, puis ex√©cuter le script ``i2samp.sh``.  
* :ref:`test_piper` ‚Äî V√©rifier les langues prises en charge par **Piper TTS**.  
* :ref:`test_vosk` ‚Äî V√©rifier les langues prises en charge par **Vosk STT**.  
* :ref:`py_online_llm` ‚Äî Cette √©tape est **tr√®s importante** : obtenez votre cl√© API **OpenAI** ou **Doubao**, ou la cl√© API d‚Äôun autre LLM pris en charge.

Vous devez d√©j√† disposer de :

* Un **microphone** et un **haut-parleur** fonctionnels sur votre Pidog.  
* Une **cl√© API valide** enregistr√©e dans ``secret.py``.  
* Une connexion r√©seau stable (une **connexion filaire** est recommand√©e pour plus de stabilit√©).

----

Ex√©cuter l‚ÄôExemple
------------------

Les deux versions linguistiques sont plac√©es dans le m√™me r√©pertoire :

.. code-block:: bash

   cd ~/pidog/examples

**Version anglaise** (OpenAI GPT, instructions en anglais) :

.. code-block:: bash

   sudo python3 20_voice_active_dog_gpt.py

* LLM : ``OpenAI GPT-4o-mini``  
* TTS : ``en_US-ryan-low`` (Piper)  
* STT : Vosk (``en-us``)

Mot de r√©veil :

.. code-block::

   "Hey buddy"

---

**Version chinoise** (Doubao, instructions en chinois) :

.. code-block:: bash

   sudo python3 20_voice_active_dog_doubao_cn.py

* LLM : ``Doubao-seed-1-6-250615``  
* TTS : ``zh_CN-huayan-x_low`` (Piper)  
* STT : Vosk (``cn``)

Mot de r√©veil :

.. code-block::

   "‰Ω†Â•Ω Êó∫Ë¥¢"

.. note::

   Vous pouvez modifier le **mot de r√©veil** et le **nom du robot** dans le code :
   ``NAME = "Buddy"`` ou ``NAME = "Êó∫Ë¥¢"``  
   ``WAKE_WORD = ["hey buddy"]`` ou ``WAKE_WORD = ["‰Ω†Â•Ω Êó∫Ë¥¢"]``

----

Ce qui va se Passer
-------------------

Lorsque vous ex√©cutez cet exemple avec succ√®s :

* Le robot **attend le mot de r√©veil** (par ex. ¬´ Hey Buddy ¬ª, ¬´ ‰Ω†Â•Ω Êó∫Ë¥¢ ¬ª).  
* Lorsqu‚Äôil entend le mot de r√©veil :

  * La bande LED devient **rose (effet de respiration)** comme signal de r√©veil.  
  * Le robot vous salue avec la r√©ponse de r√©veil d√©finie ‚Äî  
    par ex. ¬´ Salut ! ¬ª (via Piper TTS).

* Il commence ensuite √† **√©couter votre voix** gr√¢ce √† Vosk STT (ou accepte une saisie clavier si activ√©).

* Apr√®s avoir reconnu ce que vous avez dit, le syst√®me :

  * Capture une **image de la cam√©ra** (car ``WITH_IMAGE = True``) et envoie votre message + image au **LLM** (OpenAI ``gpt-4o-mini``).  
  * La LED passe en **jaune (√©coute/traitement)** pendant que le mod√®le r√©fl√©chit.  
  * La r√©ponse du mod√®le est divis√©e en deux parties :

    - Texte avant ``ACTIONS:`` ‚Üí prononc√© √† voix haute.  
    - Mots-cl√©s apr√®s ``ACTIONS:`` ‚Üí associ√©s aux mouvements du robot.

  * Le robot **ex√©cute ces actions** via ``ActionFlow``.  
  * Lorsque les actions sont termin√©es, le robot **revient en posture ASSISE** et √©teint les LEDs.

* Si le **capteur ultrason d√©tecte un obstacle** √† moins de 10 cm :

  - Un message est inject√© : ``<<<D√©tection ultrason trop proche : {distance}cm>>>``  
  - Le robot recule automatiquement : ``ACTIONS: backward``  
  - **L‚Äôentr√©e image est d√©sactiv√©e** pour ce tour.

* Si le **capteur tactile est d√©clench√©** :

  - Pour un **TOUCHER AIM√â** (par ex. FRONT_TO_REAR) :

    - Injection : ``<<<Style de toucher appr√©ci√© : FRONT_TO_REAR>>>``  
    - ``ACTIONS: nod`` (r√©ponse positive)

  - Pour un **TOUCHER D√âTEST√â** (par ex. REAR_TO_FRONT) :

    - Injection : ``<<<Style de toucher d√©test√© : REAR_TO_FRONT>>>``  
    - ``ACTIONS: backward`` (r√©action d‚Äô√©vitement)

* **Cycle de vie des LEDs :**  

  - ``on_start`` ‚Üí posture ASSISE, LEDs √©teintes  
  - ``before_listen`` ‚Üí cyan (pr√™t)  
  - ``before_think`` ‚Üí jaune (en traitement)  
  - ``before_say`` ‚Üí rose (en train de parler)  
  - ``after_say`` / ``on_finish_a_round`` ‚Üí posture ASSISE, LEDs √©teintes  
  - ``on_stop`` ‚Üí arr√™t du flux d‚Äôaction et fermeture des p√©riph√©riques

**Exemple d‚Äôinteraction**

.. code-block:: text

   You: Hey Buddy
   Robot: Hi there!

   You: What do you see in front of you?
   Robot: I can see a notebook and a blue mug on the table.
   ACTIONS: think

   You: Do a little nod for me.
   Robot: Of course. Watch my majestic nod.
   ACTIONS: nod

   (Front-to-rear touch on the head)
   Robot: Ooooh, that‚Äôs nice!
   ACTIONS: nod

   (Moving too close)
   Robot: Hey hey‚Äîtoo close! Backing up for safety.
   ACTIONS: backward


----

Passer √† d‚ÄôAutres LLM ou TTS
----------------------------

Vous pouvez facilement passer √† d‚Äôautres LLM, TTS ou langues STT avec seulement quelques modifications :

* LLM pris en charge :

  * OpenAI
  * Doubao
  * Deepseek
  * Gemini
  * Qwen
  * Grok

* :ref:`test_piper` ‚Äî V√©rifiez les langues prises en charge par **Piper TTS**.  
* :ref:`test_vosk` ‚Äî V√©rifiez les langues prises en charge par **Vosk STT**.  

Pour changer, modifiez simplement la partie d‚Äôinitialisation dans le code :

.. code-block:: python

  from pidog.llm import OpenAI as LLM

  llm = LLM(
      api_key=API_KEY,
      model="gpt-4o-mini",
  )

  # D√©finir les mod√®les et les langues
  TTS_MODEL = "en_US-ryan-low"
  STT_LANGUAGE = "en-us"

----

D√©pannage
---------

* **Le robot ne r√©pond pas au mot de r√©veil**  

  * V√©rifiez si le microphone fonctionne.  
  * Assurez-vous que ``WAKE_ENABLE = True``.  
  * Ajustez le mot de r√©veil pour correspondre √† votre prononciation.

* **Aucun son ne sort du haut-parleur**
 
  * V√©rifiez la configuration du mod√®le TTS.  
  * Testez Piper ou Espeak manuellement.  
  * V√©rifiez la connexion et le volume du haut-parleur.

* **Erreur de cl√© API ou d√©lai d‚Äôexpiration (timeout)** 
 
  * V√©rifiez votre cl√© dans ``secret.py``.  
  * Assurez la connexion r√©seau.  
  * Confirmez que le LLM est pris en charge.

* **Le capteur ultrason se d√©clenche de mani√®re inattendue.**  

  * V√©rifiez la hauteur et l‚Äôangle d‚Äôinstallation du capteur.  
  * Ajustez le seuil de distance ``TOO_CLOSE`` dans le code.
