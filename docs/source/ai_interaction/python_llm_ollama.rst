.. note::

    Bonjour, bienvenue dans la communaut√© SunFounder Raspberry Pi & Arduino & ESP32 Enthusiasts sur Facebook ! Plongez plus profond√©ment dans l‚Äôunivers de Raspberry Pi, Arduino et ESP32 avec d‚Äôautres passionn√©s.

    **Pourquoi rejoindre ?**

    - **Support d'experts** : R√©solvez les probl√®mes apr√®s-vente et relevez les d√©fis techniques avec l'aide de notre communaut√© et de notre √©quipe.
    - **Apprendre et partager** : √âchangez des astuces et des tutoriels pour am√©liorer vos comp√©tences.
    - **Aper√ßus exclusifs** : B√©n√©ficiez d‚Äôun acc√®s anticip√© aux annonces de nouveaux produits et √† des avant-premi√®res.
    - **R√©ductions sp√©ciales** : Profitez de remises exclusives sur nos produits les plus r√©cents.
    - **Promotions festives et concours** : Participez √† des concours et √† des promotions sp√©ciales lors des f√™tes.

    üëâ Pr√™t √† explorer et √† cr√©er avec nous ? Cliquez sur [|link_sf_facebook|] et rejoignez-nous d√®s aujourd'hui !

17. Conversation Textuelle avec Ollama
======================================

Dans cette le√ßon, vous allez apprendre √† utiliser **Ollama**, un outil permettant d‚Äôex√©cuter localement de grands mod√®les de langage et de vision.  
Nous vous montrerons comment installer Ollama, t√©l√©charger un mod√®le et connecter Pidog √† celui-ci.

Avant de Commencer
------------------

Assurez-vous d‚Äôavoir termin√© :

* :ref:`install_all_modules` ‚Äî Installer les modules ``robot-hat``, ``vilib``, ``Pidog``, puis ex√©cuter le script ``i2samp.sh``.

.. _download_ollama:

1. Installer Ollama (LLM) et T√©l√©charger un Mod√®le
--------------------------------------------------

Vous pouvez choisir o√π installer **Ollama** :

* Sur votre Raspberry Pi (ex√©cution locale)  
* Ou sur un autre ordinateur (Mac/Windows/Linux) dans le **m√™me r√©seau local**

**Mod√®les recommand√©s selon le mat√©riel**

Vous pouvez choisir n‚Äôimporte quel mod√®le disponible sur |link_ollama_hub|.  
Les mod√®les sont propos√©s en diff√©rentes tailles (3B, 7B, 13B, 70B...).  
Les mod√®les plus petits s‚Äôex√©cutent plus rapidement et n√©cessitent moins de m√©moire, tandis que les plus grands offrent une meilleure qualit√© mais demandent un mat√©riel plus puissant.

Consultez le tableau ci-dessous pour d√©terminer quelle taille de mod√®le convient √† votre appareil.

.. list-table::
   :header-rows: 1
   :widths: 20 20 40

   * - Taille du mod√®le
     - RAM minimale requise
     - Mat√©riel recommand√©
   * - ~3B param√®tres
     - 8 Go (16 Go pr√©f√©rable)
     - Raspberry Pi 5 (16 Go) ou PC/Mac de milieu de gamme
   * - ~7B param√®tres
     - 16 Go+
     - Pi 5 (16 Go, limite) ou PC/Mac de milieu de gamme
   * - ~13B param√®tres
     - 32 Go+
     - PC/Mac de bureau avec beaucoup de RAM
   * - 30B+ param√®tres
     - 64 Go+
     - Station de travail / serveur / GPU recommand√©
   * - 70B+ param√®tres
     - 128 Go+
     - Serveur haut de gamme avec plusieurs GPU

**Installation sur Raspberry Pi**

Si vous souhaitez ex√©cuter Ollama directement sur votre Raspberry Pi :

* Utilisez un **syst√®me Raspberry Pi OS 64 bits**  
* Fortement recommand√© : **Raspberry Pi 5 (16 Go de RAM)**

Ex√©cutez les commandes suivantes :

.. code-block:: bash

   # Installer Ollama
   curl -fsSL https://ollama.com/install.sh | sh

   # T√©l√©charger un mod√®le l√©ger (pour tester)
   ollama pull llama3.2:3b

   # Test rapide (tapez 'hi' puis Entr√©e)
   ollama run llama3.2:3b

   # Lancer l‚ÄôAPI (port par d√©faut 11434)
   # Astuce : d√©finir OLLAMA_HOST=0.0.0.0 pour autoriser l‚Äôacc√®s depuis le LAN
   OLLAMA_HOST=0.0.0.0 ollama serve

**Installation sur Mac / Windows / Linux (Application de Bureau)**

1. T√©l√©chargez et installez Ollama depuis |link_ollama|  

   .. image:: img/llm_ollama_download.png

2. Ouvrez l‚Äôapplication Ollama, allez dans le **s√©lecteur de mod√®les**, et utilisez la barre de recherche pour trouver un mod√®le. Par exemple, tapez ``llama3.2:3b`` (un mod√®le petit et l√©ger pour commencer).

   .. image:: img/llm_ollama_choose.png

3. Une fois le t√©l√©chargement termin√©, tapez quelque chose de simple comme ¬´ Hi ¬ª dans la fen√™tre de chat, Ollama commencera automatiquement √† le t√©l√©charger lors de la premi√®re utilisation.

   .. image:: img/llm_olama_llama_download.png

4. Allez dans **Settings** ‚Üí activez **Expose Ollama to the network**. Cela permet √† votre Raspberry Pi de s‚Äôy connecter via le r√©seau local.

   .. image:: img/llm_olama_windows_enable.png

.. warning::

   Si vous voyez une erreur du type :

   ``Error: model requires more system memory ...``

   Cela signifie que le mod√®le est trop grand pour votre machine.  
   Utilisez un **mod√®le plus petit** ou passez sur un ordinateur avec plus de RAM.

2. Tester Ollama
----------------

Une fois Ollama install√© et votre mod√®le pr√™t, vous pouvez le tester rapidement avec une boucle de chat minimale.

**√âtapes**

#. Cr√©ez un nouveau fichier :

   .. code-block:: bash

      cd ~/pidog/examples
      nano test_llm_ollama.py

#. Collez le code suivant et enregistrez (``Ctrl+X`` ‚Üí ``Y`` ‚Üí ``Entr√©e``) :

   .. code-block:: python
 
      from pidog.llm import Ollama
 
      INSTRUCTIONS = "You are a helpful assistant."
      WELCOME = "Hello, I am a helpful assistant. How can I help you?"
 
      # If Ollama runs on the same Raspberry Pi, use "localhost".
      # If it runs on another computer in your LAN, replace with that computer's IP address.
      llm = Ollama(
          ip="localhost",
          model="llama3.2:3b"   # you can replace with any model
      )
 
      # Basic configuration
      llm.set_max_messages(20)
      llm.set_instructions(INSTRUCTIONS)
      llm.set_welcome(WELCOME)
 
      print(WELCOME)
 
      while True:
          text = input(">>> ")
          if text.strip().lower() in {"exit", "quit"}:
              break
 
          # Response with streaming output
          response = llm.prompt(text, stream=True)
          for token in response:
              if token:
                  print(token, end="", flush=True)
          print("")

#. Ex√©cutez le programme :

   .. code-block:: bash

      python3 test_llm_ollama.py

#. Vous pouvez maintenant discuter avec Pidog directement depuis le terminal.

   * Vous pouvez choisir **n‚Äôimporte quel mod√®le** disponible sur |link_ollama_hub|, mais les mod√®les plus petits (par ex. ``moondream:1.8b``, ``phi3:mini``) sont recommand√©s si vous n‚Äôavez que 8 √† 16 Go de RAM.  
   * Assurez-vous que le mod√®le sp√©cifi√© dans le code correspond bien √† celui que vous avez d√©j√† t√©l√©charg√© dans Ollama.  
   * Tapez ``exit`` ou ``quit`` pour arr√™ter le programme.  
   * Si vous ne pouvez pas vous connecter, assurez-vous qu‚ÄôOllama est en cours d‚Äôex√©cution et que les deux appareils sont sur le m√™me LAN si vous utilisez un h√¥te distant.

D√©pannage
---------

* **J‚Äôobtiens une erreur du type : `model requires more system memory ...`.**

  * Cela signifie que le mod√®le est trop volumineux pour votre appareil.  
  * Utilisez un mod√®le plus petit comme ``moondream:1.8b`` ou ``granite3.2-vision:2b``.  
  * Ou passez sur une machine avec plus de RAM et exposez Ollama au r√©seau.

* **Le code ne parvient pas √† se connecter √† Ollama (connexion refus√©e).**

  V√©rifiez les points suivants :
  
  * Assurez-vous qu‚ÄôOllama est en cours d‚Äôex√©cution (``ollama serve`` ou que l‚Äôapplication de bureau est ouverte).  
  * Si vous utilisez un ordinateur distant, activez **Expose to network** dans les param√®tres d‚ÄôOllama.  
  * V√©rifiez que l‚Äôadresse ``ip="..."`` dans votre code correspond bien √† l‚Äôadresse IP LAN correcte.  
  * Confirmez que les deux appareils sont bien sur le m√™me r√©seau local.
