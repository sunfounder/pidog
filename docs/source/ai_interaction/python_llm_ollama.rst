.. note::

    Hola, ¬°bienvenido a la comunidad de entusiastas de SunFounder Raspberry Pi & Arduino & ESP32 en Facebook! Sum√©rgete a√∫n m√°s en el mundo de Raspberry Pi, Arduino y ESP32 con otros apasionados.

    **¬øPor qu√© unirse?**

    - **Soporte de Expertos**: Resuelve problemas post-venta y desaf√≠os t√©cnicos con la ayuda de nuestra comunidad y equipo.
    - **Aprende y Comparte**: Intercambia consejos y tutoriales para mejorar tus habilidades.
    - **Avances Exclusivos**: Obt√©n acceso anticipado a los anuncios de nuevos productos y a vistas previas.
    - **Descuentos Especiales**: Disfruta de descuentos exclusivos en nuestros productos m√°s recientes.
    - **Promociones y Sorteos Festivos**: Participa en sorteos y promociones especiales durante las festividades.

    üëâ ¬øListo para explorar y crear con nosotros? Haz clic en [|link_sf_facebook|] y √∫nete hoy mismo.

17. Conversaci√≥n de Texto con Ollama
============================================

En esta lecci√≥n, aprender√°s a usar **Ollama**, una herramienta para ejecutar modelos grandes de lenguaje y visi√≥n de forma local.  
Te mostraremos c√≥mo instalar Ollama, descargar un modelo y conectar Pidog a √©l.  

Antes de Comenzar
------------------------

Aseg√∫rate de haber completado:

* :ref:`install_all_modules` ‚Äî Instalar los m√≥dulos ``robot-hat``, ``vilib``, ``Pidog`` y luego ejecutar el script ``i2samp.sh``.

.. _download_ollama:

1. Instalar Ollama (LLM) y Descargar Modelo
-------------------------------------------------

Puedes elegir d√≥nde instalar **Ollama**: 

* En tu Raspberry Pi (ejecuci√≥n local)  
* O en otra computadora (Mac/Windows/Linux) en la **misma red local**  

**Modelos recomendados seg√∫n el hardware**

Puedes elegir cualquier modelo disponible en |link_ollama_hub|.  
Los modelos vienen en diferentes tama√±os (3B, 7B, 13B, 70B...).  
Los modelos m√°s peque√±os se ejecutan m√°s r√°pido y requieren menos memoria, mientras que los m√°s grandes ofrecen mejor calidad pero necesitan hardware m√°s potente.

Consulta la siguiente tabla para decidir qu√© tama√±o de modelo se adapta mejor a tu dispositivo.

.. list-table::
   :header-rows: 1
   :widths: 20 20 40

   * - Tama√±o del modelo
     - RAM m√≠nima requerida
     - Hardware recomendado
   * - ~3B par√°metros
     - 8GB (mejor con 16GB)
     - Raspberry Pi 5 (16GB) o PC/Mac de gama media
   * - ~7B par√°metros
     - 16GB+
     - Pi 5 (16GB, apenas usable) o PC/Mac de gama media
   * - ~13B par√°metros
     - 32GB+
     - PC de escritorio / Mac con alta RAM
   * - 30B+ par√°metros
     - 64GB+
     - Estaci√≥n de trabajo / Servidor / GPU recomendada
   * - 70B+ par√°metros
     - 128GB+
     - Servidor de gama alta con m√∫ltiples GPU

**Instalar en Raspberry Pi**

Si deseas ejecutar Ollama directamente en tu Raspberry Pi:

* Usa un **Raspberry Pi OS de 64 bits**  
* Muy recomendado: **Raspberry Pi 5 (16GB RAM)**  

Ejecuta los siguientes comandos:

.. code-block:: bash

   # Instalar Ollama
   curl -fsSL https://ollama.com/install.sh | sh

   # Descargar un modelo liviano (bueno para pruebas)
   ollama pull llama3.2:3b

   # Prueba r√°pida (escribe 'hi' y presiona Enter)
   ollama run llama3.2:3b

   # Servir la API (puerto por defecto 11434)
   # Consejo: establece OLLAMA_HOST=0.0.0.0 para permitir acceso desde la LAN
   OLLAMA_HOST=0.0.0.0 ollama serve

**Instalar en Mac / Windows / Linux (Aplicaci√≥n de Escritorio)**

1. Descarga e instala Ollama desde |link_ollama|  

   .. image:: img/llm_ollama_download.png

2. Abre la aplicaci√≥n Ollama, ve a **Model Selector** y usa la barra de b√∫squeda para encontrar un modelo. Por ejemplo, escribe ``llama3.2:3b`` (un modelo peque√±o y ligero para comenzar).  

   .. image:: img/llm_ollama_choose.png

3. Una vez completada la descarga, escribe algo simple como ‚ÄúHi‚Äù en la ventana de chat. Ollama iniciar√° la descarga autom√°ticamente la primera vez que lo uses.

   .. image:: img/llm_olama_llama_download.png

4. Ve a **Settings** ‚Üí activa **Expose Ollama to the network**. Esto permite que tu Raspberry Pi se conecte a trav√©s de la red local (LAN).  

   .. image:: img/llm_olama_windows_enable.png

.. warning::

   Si ves un error como:

   ``Error: model requires more system memory ...``

   El modelo es demasiado grande para tu m√°quina.  
   Usa un **modelo m√°s peque√±o** o cambia a una computadora con m√°s RAM.

2. Probar Ollama
---------------------------

Una vez que Ollama est√© instalado y tu modelo est√© listo, puedes probarlo r√°pidamente con un bucle de chat m√≠nimo.

**Pasos**

#. Crea un nuevo archivo:

   .. code-block:: bash
 
      cd ~/pidog/examples
      nano test_llm_ollama.py

#. Pega el siguiente c√≥digo y guarda (``Ctrl+X`` ‚Üí ``Y`` ‚Üí ``Enter``):

   .. code-block:: python
 
      from pidog.llm import Ollama
 
      INSTRUCTIONS = "You are a helpful assistant."
      WELCOME = "Hello, I am a helpful assistant. How can I help you?"
 
      # If Ollama runs on the same Raspberry Pi, use "localhost".
      # If it runs on another computer in your LAN, replace with that computer's IP address.
      llm = Ollama(
          ip="localhost",
          model="llama3.2:3b"   # you can replace with any model
      )
 
      # Basic configuration
      llm.set_max_messages(20)
      llm.set_instructions(INSTRUCTIONS)
      llm.set_welcome(WELCOME)
 
      print(WELCOME)
 
      while True:
          text = input(">>> ")
          if text.strip().lower() in {"exit", "quit"}:
              break
 
          # Response with streaming output
          response = llm.prompt(text, stream=True)
          for token in response:
              if token:
                  print(token, end="", flush=True)
          print("")

#. Ejecuta el programa:

   .. code-block:: bash
 
      python3 test_llm_ollama.py
 
#. Ahora puedes chatear con Pidog directamente desde la terminal.

   * Puedes elegir **cualquier modelo** disponible en |link_ollama_hub|, pero se recomiendan modelos m√°s peque√±os (por ejemplo, ``moondream:1.8b``, ``phi3:mini``) si solo tienes 8‚Äì16GB de RAM.  
   * Aseg√∫rate de que el modelo que especifiques en el c√≥digo coincida con el que ya descargaste en Ollama.  
   * Escribe ``exit`` o ``quit`` para detener el programa.  
   * Si no puedes conectarte, aseg√∫rate de que Ollama est√© en ejecuci√≥n y que ambos dispositivos est√©n en la misma LAN si usas un host remoto.

Soluci√≥n de Problemas
-------------------------------

* **Recibo un error como: `model requires more system memory ...`.**

  * Esto significa que el modelo es demasiado grande para tu dispositivo.  
  * Usa un modelo m√°s peque√±o como ``moondream:1.8b`` o ``granite3.2-vision:2b``.  
  * O cambia a una m√°quina con m√°s RAM y exp√≥n Ollama a la red.

* **El c√≥digo no puede conectarse a Ollama (conexi√≥n rechazada).**

  Verifica lo siguiente:
  
  * Aseg√∫rate de que Ollama est√© ejecut√°ndose (``ollama serve`` o que la aplicaci√≥n de escritorio est√© abierta).  
  * Si usas una computadora remota, activa **Expose to network** en la configuraci√≥n de Ollama.  
  * Verifica que la direcci√≥n ``ip="..."`` en tu c√≥digo coincida con la IP correcta de la LAN.  
  * Confirma que ambos dispositivos est√©n en la misma red local.
