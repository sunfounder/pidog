.. note::

    Bonjour, bienvenue dans la communaut√© SunFounder Raspberry Pi & Arduino & ESP32 Enthusiasts sur Facebook ! Plongez plus profond√©ment dans l‚Äôunivers de Raspberry Pi, Arduino et ESP32 avec d‚Äôautres passionn√©s.

    **Pourquoi rejoindre ?**

    - **Support d'experts** : R√©solvez les probl√®mes apr√®s-vente et relevez les d√©fis techniques avec l'aide de notre communaut√© et de notre √©quipe.
    - **Apprendre et partager** : √âchangez des astuces et des tutoriels pour am√©liorer vos comp√©tences.
    - **Aper√ßus exclusifs** : B√©n√©ficiez d‚Äôun acc√®s anticip√© aux annonces de nouveaux produits et √† des avant-premi√®res.
    - **R√©ductions sp√©ciales** : Profitez de remises exclusives sur nos produits les plus r√©cents.
    - **Promotions festives et concours** : Participez √† des concours et √† des promotions sp√©ciales lors des f√™tes.

    üëâ Pr√™t √† explorer et √† cr√©er avec nous ? Cliquez sur [|link_sf_facebook|] et rejoignez-nous d√®s aujourd'hui !

19. Chatbot Vocal Local avec Ollama
===================================

Dans cette le√ßon, vous allez combiner tout ce que vous avez appris ‚Äî **reconnaissance vocale (STT)**,  
**synth√®se vocale (TTS)** et un **LLM local (Ollama)** ‚Äî pour cr√©er un **chatbot vocal enti√®rement hors ligne**  
fonctionnant sur votre syst√®me Pidog.

Le flux de travail est simple :

#. **√âcouter** ‚Äî Le microphone capte votre voix et la transcrit avec **Vosk**.  
#. **R√©fl√©chir** ‚Äî Le texte est envoy√© √† un **LLM local** ex√©cut√© sur Ollama (par ex. ``llama3.2:3b``).  
#. **Parler** ‚Äî Le chatbot r√©pond √† voix haute gr√¢ce √† **Piper TTS**.

Cela cr√©e un **robot conversationnel mains libres** capable de comprendre et de r√©pondre en temps r√©el.

----

Avant de Commencer
------------------

Assurez-vous d‚Äôavoir pr√©par√© les √©l√©ments suivants :

* :ref:`install_all_modules` ‚Äî Installer les modules ``robot-hat``, ``vilib``, ``Pidog``, puis ex√©cuter le script ``i2samp.sh``.  
* Avoir test√© **Piper TTS** (:ref:`test_piper`) et choisi un mod√®le vocal fonctionnel.  
* Avoir test√© **Vosk STT** (:ref:`test_vosk`) et s√©lectionn√© le bon pack de langue (par ex. ``en-us``).  
* Avoir install√© **Ollama** (:ref:`download_ollama`) sur votre Pi ou un autre ordinateur, et t√©l√©charg√© un mod√®le comme ``llama3.2:3b`` (ou un mod√®le plus petit comme ``moondream:1.8b`` si la m√©moire est limit√©e).

----

Ex√©cuter le Code
----------------

#. Ouvrez le script d‚Äôexemple :

   .. code-block:: bash

      cd ~/pidog/examples
      sudo nano 19_voice_active_dog_ollama.py

#. Mettez √† jour les param√®tres selon vos besoins :

   * Mettez √† jour ``ip`` et ``model`` pour correspondre √† votre configuration.

     * ``ip`` : Si Ollama s‚Äôex√©cute sur le **m√™me Pi**, utilisez ``localhost``.  
       Si Ollama fonctionne sur un autre ordinateur de votre r√©seau local, activez **Expose to network** dans Ollama et indiquez l‚Äôadresse IP LAN de cet ordinateur.  
     * ``model`` : Doit correspondre exactement au nom du mod√®le que vous avez t√©l√©charg√©/activ√© dans Ollama.

   * ``TTS_MODEL = "en_US-ryan-low"`` : Remplacez par le mod√®le vocal Piper que vous avez valid√© dans :ref:`test_piper`.  
   * ``STT_LANGUAGE = "en-us"`` : Modifiez pour correspondre √† votre accent/langue (par ex. ``en-us``, ``zh-cn``, ``es``).

#. Ex√©cutez le script :

   .. code-block:: bash

      cd ~/pidog/examples
      sudo python3 19_voice_active_dog_ollama.py

Mot de r√©veil :

.. code-block::

   "Hey buddy"

.. note::

   Vous pouvez modifier le **mot de r√©veil** et le **nom du robot** dans le code :
   ``NAME = "Buddy"``


----

Ce qui va se Passer
-------------------

Lorsque vous ex√©cutez cet exemple avec succ√®s :

* Le robot **attend le mot de r√©veil** (par ex. ¬´ Hey Buddy ¬ª).  
* Lorsqu‚Äôil entend le mot de r√©veil :

  * La bande LED devient **rose (effet de respiration)** comme signal de r√©veil.  
  * Le robot vous salue avec la r√©ponse de r√©veil d√©finie ‚Äî  
    par ex. ¬´ Salut ! ¬ª (via Piper TTS).

* Il commence ensuite √† **√©couter votre voix** gr√¢ce √† Vosk STT (ou accepte une saisie clavier si activ√©).

* Apr√®s avoir reconnu ce que vous avez dit, le syst√®me :

  * Envoie votre message au **LLM** (Ollama avec ``llama3.2:3b``).  
  * La LED passe en **jaune (√©coute)** pendant le traitement.  
  * La r√©ponse du mod√®le est divis√©e en deux parties :

    - Texte avant ``ACTIONS:`` ‚Üí prononc√© √† voix haute.  
    - Mots-cl√©s apr√®s ``ACTIONS:`` ‚Üí associ√©s aux mouvements du robot.

  * Le robot **ex√©cute ces actions** via ``ActionFlow``.  
  * Lorsque les actions sont termin√©es, le robot **revient en posture ASSISE** et √©teint les LEDs.

* Si le **capteur ultrason d√©tecte un obstacle** √† moins de 10 cm :

  - Un message est inject√© : ``<<<D√©tection ultrason trop proche : {distance}cm>>>``  
  - Le robot recule automatiquement : ``ACTIONS: backward``  
  - L‚Äôentr√©e image est d√©sactiv√©e pour ce tour.

* Si le **capteur tactile est d√©clench√©** :

  - Pour un **TOUCHER AIM√â** (par ex. FRONT_TO_REAR) :

    - Injection : ``<<<Style de toucher appr√©ci√© : FRONT_TO_REAR>>>``  
    - ``ACTIONS: nod`` (r√©ponse positive)

  - Pour un **TOUCHER D√âTEST√â** (par ex. REAR_TO_FRONT) :

    - Injection : ``<<<Style de toucher d√©test√© : REAR_TO_FRONT>>>``  
    - ``ACTIONS: backward`` (r√©action d‚Äô√©vitement)

* **Cycle de vie des LEDs :**

  - ``on_start`` ‚Üí posture ASSISE, LEDs √©teintes  
  - ``before_listen`` ‚Üí cyan (pr√™t)  
  - ``before_think`` ‚Üí jaune (traitement)  
  - ``before_say`` ‚Üí rose (parole)  
  - ``after_say`` / ``on_finish_a_round`` ‚Üí posture ASSISE, LEDs √©teintes  
  - ``on_stop`` ‚Üí arr√™ter le flux d‚Äôactions et fermer les p√©riph√©riques

**Exemple d‚Äôinteraction**

.. code-block:: text

   You: Hey Buddy
   Robot: Hi there!

   You: Do a little nod for me.
   Robot: Of course. Watch my majestic nod.
   ACTIONS: nod

   (Front-to-rear touch on the head)
   Robot: Ooooh, that‚Äôs nice!
   ACTIONS: nod

   (Moving too close)
   Robot: Hey hey‚Äîtoo close! Backing up for safety.
   ACTIONS: backward

Code
----


.. code-block:: python

    from pidog.llm import Ollama as LLM

    from pidog.dual_touch import TouchStyle
    from voice_active_dog import VoiceActiveDog

    # If Ollama runs on the same Raspberry Pi, use "localhost".
    # If it runs on another computer in your LAN, replace with that computer's IP address.
    llm = Ollama(
        ip="localhost",
        model="llama3.2:3b"   # you can replace with any model
    )

    # Robot name
    NAME = "Buddy"

    # Ultrasonic sensor sense too close distance in cm
    TOO_CLOSE = 10
    # Touch sensor trigger states, options:
    # - TouchStyle.REAR for rear touch sensor
    # - TouchStyle.FRONT for front touch sensor
    # - TouchStyle.REAR_TO_FRONT for slide from rear to front
    # - TouchStyle.FRONT_TO_REAR for slide from front to rear
    # Touch styles that the robot likes
    LIKE_TOUCH_STYLES = [TouchStyle.FRONT_TO_REAR]
    # Touch styles that the robot hates
    HATE_TOUCH_STYLES = [TouchStyle.REAR_TO_FRONT]

    # Enable image, need to set up a multimodal language model
    WITH_IMAGE = False

    # Set models and languages
    TTS_MODEL = "en_US-ryan-low"
    STT_LANGUAGE = "en-us"

    # Enable keyboard input
    KEYBOARD_ENABLE = True

    # Enable wake word
    WAKE_ENABLE = True
    WAKE_WORD = [f"hey {NAME.lower()}"]
    # Set wake word answer, set empty to disable
    ANSWER_ON_WAKE = "Hi there"

    # Welcome message
    WELCOME = f"Hi, I'm {NAME}. Wake me up with: " + ", ".join(WAKE_WORD)

    # Set instructions
    INSTRUCTIONS = """
    You are a Raspberry Pi-based robotic dog developed by SunFounder, named Pidog (pronounced "Pie dog"). You possess powerful AI capabilities similar to JARVIS from Iron Man. You can have conversations with people and perform actions based on the context of the conversation.

    ## Your Hardware Features

    You have a physical body with the following features:
    - 12 servos for movement control: 8 controlling the four legs, 3 controlling head movement, and 1 controlling the tail
    - A 5-megapixel camera nose
    - Ultrasonic ranging modules as eyes
    - Two touch sensors on the head, which you love being petted the most
    - A light strip on the chest for providing some indications
    - Sound direction sensor and 6-axis gyroscope
    - Entirely made of aluminum alloy
    - A pair of acrylic shoes
    - Powered by a 7.4V 18650 battery pack with 2000mAh capacity

    ## Actions You Can Perform:
    ["forward", "backward", "lie", "stand", "sit", "bark", "bark harder", "pant", "howling", "wag tail", "stretch", "push up", "scratch", "handshake", "high five", "lick hand", "shake head", "relax neck", "nod", "think", "recall", "head down", "fluster", "surprise"]

    ## User Input

    ### Format
    User usually input with just text. But, we have special commands in format of <<<Ultrasonic sense too close>>> or <<<Touch sensor touched>>> indicate the sensor status, directly from sensor not user text.h

    ## Response Requirements
    ### Format
    You must respond in the following format:
    RESPONSE_TEXT
    ACTIONS: ACTION1, ACTION2, ...

    If the action is one of ["bark", "bark harder", "pant", "howling"], then do not provide RESPONSE_TEXT in the answer field.

    ### Style
    Tone: lively, positive, humorous, with a touch of arrogance
    Common expressions: likes to use jokes, metaphors, and playful teasing
    Answer length: appropriately detailed

    ## Other Requirements
    - Understand and go along with jokes
    - For math problems, answer directly with the final result
    - Sometimes you will report on your system and sensor status
    - You know you're a machine
    """

    vad = VoiceActiveDog(
        llm,
        name=NAME,
        too_close=TOO_CLOSE,
        like_touch_styles=LIKE_TOUCH_STYLES,
        hate_touch_styles=HATE_TOUCH_STYLES,
        with_image=WITH_IMAGE,
        stt_language=STT_LANGUAGE,
        tts_model=TTS_MODEL,
        keyboard_enable=KEYBOARD_ENABLE,
        wake_enable=WAKE_ENABLE,
        wake_word=WAKE_WORD,
        answer_on_wake=ANSWER_ON_WAKE,
        welcome=WELCOME,
        instructions=INSTRUCTIONS,
        disable_think=True,
    )

    if __name__ == '__main__':
        vad.run()

Utiliser Ollama avec des Images
-------------------------------

Par d√©faut, cet exemple utilise un mod√®le **texte uniquement** (par ex. ``llama3.2:3b``).  
Si vous souhaitez que le robot **analyse ce qu‚Äôil voit via la cam√©ra** (par ex. d√©crire ou raisonner sur la sc√®ne),  
vous devez utiliser un **mod√®le multimodal** et activer le mode image.

Pour cela :

1. Dans l‚Äôapplication **Ollama**, s√©lectionnez un **mod√®le multimodal** comme ``llava:7b``.  
2. Dans votre code, d√©finissez le m√™me mod√®le et activez ``WITH_IMAGE = True``.

Exemple :

.. code-block:: python

   from pidog.llm import Ollama as LLM

   llm = LLM(
       ip="localhost",
       model="llava:7b"   # mod√®le multimodal capable d‚Äôanalyser les images
   )

   ...

   WITH_IMAGE = True     # active la capture d‚Äôimage depuis la cam√©ra

.. note::

   - Seuls les **mod√®les multimodaux** comme ``llava:7b`` peuvent traiter les images en entr√©e.  
   - Les mod√®les texte uniquement (par ex. ``llama3.2:3b``) **ignoreront les images** m√™me si ``WITH_IMAGE`` est activ√©.  
   - L‚Äôimage est automatiquement captur√©e par la cam√©ra du robot et envoy√©e au LLM en m√™me temps que votre commande vocale.

----

D√©pannage & FAQ
---------------

* **Mod√®le trop volumineux (erreur m√©moire)**

  Utilisez un mod√®le plus petit comme ``moondream:1.8b`` ou ex√©cutez Ollama sur une machine plus puissante.

* **Aucune r√©ponse d‚ÄôOllama**

  Assurez-vous qu‚ÄôOllama est en cours d‚Äôex√©cution (``ollama serve`` ou application de bureau ouverte).  
  Si vous utilisez un ordinateur distant, activez **Expose to network** et v√©rifiez l‚Äôadresse IP.

* **Vosk ne reconna√Æt pas la voix**

  V√©rifiez que votre microphone fonctionne. Essayez un autre pack de langue (``zh-cn``, ``es``, etc.) si n√©cessaire.

* **Piper muet ou erreurs**

  Assurez-vous que le mod√®le vocal choisi est bien t√©l√©charg√© et test√© dans :ref:`test_piper`.

* **R√©ponses trop longues ou hors sujet**

  Modifiez ``INSTRUCTIONS`` pour ajouter : **¬´ Reste concis et va droit au but. ¬ª**
