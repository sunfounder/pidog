.. note::

    Hallo und willkommen in der SunFounder Raspberry Pi & Arduino & ESP32 Enthusiasten-Gemeinschaft auf Facebook! Tauchen Sie tiefer ein in die Welt von Raspberry Pi, Arduino und ESP32 mit anderen Enthusiasten.

    **Warum beitreten?**

    - **Expertenunterst√ºtzung**: L√∂sen Sie Nachverkaufsprobleme und technische Herausforderungen mit Hilfe unserer Gemeinschaft und unseres Teams.
    - **Lernen & Teilen**: Tauschen Sie Tipps und Anleitungen aus, um Ihre F√§higkeiten zu verbessern.
    - **Exklusive Vorschauen**: Erhalten Sie fr√ºhzeitigen Zugang zu neuen Produktank√ºndigungen und exklusiven Einblicken.
    - **Spezialrabatte**: Genie√üen Sie exklusive Rabatte auf unsere neuesten Produkte.
    - **Festliche Aktionen und Gewinnspiele**: Nehmen Sie an Gewinnspielen und Feiertagsaktionen teil.

    üëâ Sind Sie bereit, mit uns zu erkunden und zu erschaffen? Klicken Sie auf [|link_sf_facebook|] und treten Sie heute bei!

17. Textgespr√§che mit Ollama
============================

In dieser Lektion lernst du, wie du **Ollama** verwendest ‚Äì ein Tool zum lokalen Ausf√ºhren gro√üer Sprach- und Vision-Modelle.  
Wir zeigen dir, wie du Ollama installierst, ein Modell herunterl√§dst und Pidog damit verbindest.

Bevor du beginnst
-----------------

Stelle sicher, dass du Folgendes abgeschlossen hast:

* :ref:`install_all_modules` ‚Äî Installiere die Module ``robot-hat``, ``vilib``, ``Pidog`` und f√ºhre dann das Skript ``i2samp.sh`` aus.

.. _download_ollama:

1. Ollama (LLM) installieren und Modell herunterladen
-----------------------------------------------------

Du kannst entscheiden, wo du **Ollama** installieren m√∂chtest:

* Auf deinem Raspberry Pi (lokale Ausf√ºhrung)  
* Oder auf einem anderen Computer (Mac/Windows/Linux) im **gleichen lokalen Netzwerk**

**Empfohlene Modelle vs. Hardware**

Du kannst jedes Modell verwenden, das auf |link_ollama_hub| verf√ºgbar ist.  
Modelle gibt es in verschiedenen Gr√∂√üen (3B, 7B, 13B, 70B ...).  
Kleinere Modelle laufen schneller und ben√∂tigen weniger Speicher, w√§hrend gr√∂√üere Modelle bessere Qualit√§t bieten, aber leistungsst√§rkere Hardware erfordern.

Sieh dir die Tabelle unten an, um zu entscheiden, welche Modellgr√∂√üe zu deinem Ger√§t passt.

.. list-table::
   :header-rows: 1
   :widths: 20 20 40

   * - Modellgr√∂√üe
     - Mindest-RAM erforderlich
     - Empfohlene Hardware
   * - ~3B Parameter
     - 8GB (16GB besser)
     - Raspberry Pi 5 (16GB) oder Mittelklasse-PC/Mac
   * - ~7B Parameter
     - 16GB+
     - Pi 5 (16GB, gerade ausreichend) oder Mittelklasse-PC/Mac
   * - ~13B Parameter
     - 32GB+
     - Desktop-PC / Mac mit viel RAM
   * - 30B+ Parameter
     - 64GB+
     - Workstation / Server / GPU empfohlen
   * - 70B+ Parameter
     - 128GB+
     - High-End-Server mit mehreren GPUs

**Installation auf Raspberry Pi**

Wenn du Ollama direkt auf deinem Raspberry Pi ausf√ºhren m√∂chtest:

* Verwende ein **64-Bit Raspberry Pi OS**  
* Dringend empfohlen: **Raspberry Pi 5 (16GB RAM)**

F√ºhre die folgenden Befehle aus:

.. code-block:: bash

   # Ollama installieren
   curl -fsSL https://ollama.com/install.sh | sh

   # Ein leichtgewichtiges Modell herunterladen (gut zum Testen)
   ollama pull llama3.2:3b

   # Kurzer Testlauf (Tippe 'hi' und dr√ºcke Enter)
   ollama run llama3.2:3b

   # API bereitstellen (Standardport 11434)
   # Tipp: setze OLLAMA_HOST=0.0.0.0, um Zugriff aus dem LAN zu erlauben
   OLLAMA_HOST=0.0.0.0 ollama serve

**Installation auf Mac / Windows / Linux (Desktop-App)**

1. Lade Ollama von |link_ollama| herunter und installiere es.

   .. image:: img/llm_ollama_download.png

2. √ñffne die Ollama-App, gehe zum **Model Selector** und verwende die Suchleiste, um ein Modell zu finden. Gib zum Beispiel ``llama3.2:3b`` ein (ein kleines und leichtgewichtiges Modell zum Starten).

   .. image:: img/llm_ollama_choose.png

3. Nachdem der Download abgeschlossen ist, gib etwas Einfaches wie ‚ÄûHi‚Äú in das Chatfenster ein. Ollama l√§dt das Modell beim ersten Gebrauch automatisch herunter.

   .. image:: img/llm_olama_llama_download.png

4. Gehe zu **Settings** ‚Üí aktiviere **Expose Ollama to the network**. Dadurch kann dein Raspberry Pi √ºber das LAN eine Verbindung herstellen.

   .. image:: img/llm_olama_windows_enable.png

.. warning::

   Wenn du eine Fehlermeldung wie

   ``Error: model requires more system memory ...``

   erh√§ltst, ist das Modell zu gro√ü f√ºr dein Ger√§t.  
   Verwende ein **kleineres Modell** oder wechsle zu einem Computer mit mehr RAM.

2. Ollama testen
----------------

Sobald Ollama installiert ist und dein Modell bereitsteht, kannst du es mit einer minimalen Chat-Schleife schnell testen.

**Schritte**

#. Erstelle eine neue Datei:

   .. code-block:: bash

      cd ~/pidog/examples
      nano test_llm_ollama.py

#. F√ºge den folgenden Code ein und speichere (``Ctrl+X`` ‚Üí ``Y`` ‚Üí ``Enter``):

   .. code-block:: python
 
      from pidog.llm import Ollama
 
      INSTRUCTIONS = "You are a helpful assistant."
      WELCOME = "Hello, I am a helpful assistant. How can I help you?"
 
      # If Ollama runs on the same Raspberry Pi, use "localhost".
      # If it runs on another computer in your LAN, replace with that computer's IP address.
      llm = Ollama(
          ip="localhost",
          model="llama3.2:3b"   # you can replace with any model
      )
 
      # Basic configuration
      llm.set_max_messages(20)
      llm.set_instructions(INSTRUCTIONS)
      llm.set_welcome(WELCOME)
 
      print(WELCOME)
 
      while True:
          text = input(">>> ")
          if text.strip().lower() in {"exit", "quit"}:
              break
 
          # Response with streaming output
          response = llm.prompt(text, stream=True)
          for token in response:
              if token:
                  print(token, end="", flush=True)
          print("")

#. F√ºhre das Programm aus:

   .. code-block:: bash

      python3 test_llm_ollama.py

#. Jetzt kannst du direkt √ºber das Terminal mit Pidog chatten.

   * Du kannst **jedes Modell** verwenden, das auf |link_ollama_hub| verf√ºgbar ist, aber kleinere Modelle (z. B. ``moondream:1.8b``, ``phi3:mini``) werden empfohlen, wenn du nur 8‚Äì16 GB RAM hast.  
   * Achte darauf, dass das Modell, das du im Code angibst, mit dem Modell √ºbereinstimmt, das du bereits in Ollama heruntergeladen hast.  
   * Tippe ``exit`` oder ``quit``, um das Programm zu beenden.  
   * Wenn keine Verbindung hergestellt werden kann, stelle sicher, dass Ollama l√§uft und beide Ger√§te sich im selben LAN befinden, wenn du einen Remote-Host verwendest.

Fehlerbehebung
--------------

* **Ich erhalte eine Fehlermeldung wie: `model requires more system memory ...`.**

  * Das bedeutet, dass das Modell zu gro√ü f√ºr dein Ger√§t ist.  
  * Verwende ein kleineres Modell wie ``moondream:1.8b`` oder ``granite3.2-vision:2b``.  
  * Oder wechsle zu einem Rechner mit mehr RAM und aktiviere die Netzwerkfreigabe f√ºr Ollama.

* **Der Code kann keine Verbindung zu Ollama herstellen (connection refused).**

  √úberpr√ºfe Folgendes:
  
  * Stelle sicher, dass Ollama l√§uft (``ollama serve`` oder die Desktop-App ist ge√∂ffnet).  
  * Wenn du einen Remote-Computer nutzt, aktiviere **Expose to network** in den Ollama-Einstellungen.  
  * √úberpr√ºfe, ob die im Code angegebene ``ip="..."`` mit der richtigen LAN-IP √ºbereinstimmt.  
  * Vergewissere dich, dass beide Ger√§te im selben lokalen Netzwerk sind.
